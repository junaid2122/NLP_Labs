{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f32239",
   "metadata": {},
   "source": [
    "# NLP Lab: Bag-of-Words, TF-IDF, and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09342b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (1.8.0)\n",
      "Requirement already satisfied: gensim in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (4.4.0)\n",
      "Requirement already satisfied: click in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.24.1 in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
      "Requirement already satisfied: colorama in C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%pip install nltk scikit-learn gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c52261e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da60f8",
   "metadata": {},
   "source": [
    "## Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65932d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:\n",
      "- I love natural language processing\n",
      "- Natural language processing is very interesting\n",
      "- I love learning NLP concepts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Natural language processing is very interesting\",\n",
    "    \"I love learning NLP concepts\"\n",
    "]\n",
    "\n",
    "print(\"Documents:\")\n",
    "for doc in documents:\n",
    "    print(\"-\", doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4264fa7",
   "metadata": {},
   "source": [
    "## Bag of Words – Count Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea8f7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['concepts' 'interesting' 'is' 'language' 'learning' 'love' 'natural'\n",
      " 'nlp' 'processing' 'very']\n",
      "BoW Count Matrix:\n",
      " [[0 0 0 1 0 1 1 0 1 0]\n",
      " [0 1 1 1 0 0 1 0 1 1]\n",
      " [1 0 0 0 1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_counts = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
    "print(\"BoW Count Matrix:\\n\", bow_counts.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2bef6",
   "metadata": {},
   "source": [
    "## Bag of Words – Normalized Count Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91bf3764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['concepts' 'interesting' 'is' 'language' 'learning' 'love' 'natural'\n",
      " 'nlp' 'processing' 'very']\n",
      "BoW Count Matrix:\n",
      " [[0 0 0 1 0 1 1 0 1 0]\n",
      " [0 1 1 1 0 0 1 0 1 1]\n",
      " [1 0 0 0 1 1 0 1 0 0]]\n",
      "Normalized BoW Matrix:\n",
      " [[0.         0.         0.         0.5        0.         0.5\n",
      "  0.5        0.         0.5        0.        ]\n",
      " [0.         0.40824829 0.40824829 0.40824829 0.         0.\n",
      "  0.40824829 0.         0.40824829 0.40824829]\n",
      " [0.5        0.         0.         0.         0.5        0.5\n",
      "  0.         0.5        0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Step 1: Count Occurrence\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_counts = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
    "print(\"BoW Count Matrix:\\n\", bow_counts.toarray())\n",
    "\n",
    "# Step 2: Normalized Count Occurrence (L2 normalization)\n",
    "bow_normalized = normalize(bow_counts, norm='l2')\n",
    "\n",
    "print(\"Normalized BoW Matrix:\\n\", bow_normalized.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d81e4c",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce763607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['concepts' 'interesting' 'is' 'language' 'learning' 'love' 'natural'\n",
      " 'nlp' 'processing' 'very']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.         0.5        0.         0.5\n",
      "  0.5        0.         0.5        0.        ]\n",
      " [0.         0.45954803 0.45954803 0.34949812 0.         0.\n",
      "  0.34949812 0.         0.34949812 0.45954803]\n",
      " [0.52863461 0.         0.         0.         0.52863461 0.40204024\n",
      "  0.         0.52863461 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d56f90",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0cd7fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector for 'language':\n",
      "[-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
      "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
      " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
      " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
      "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
      " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
      "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
      "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
      "  0.00180291  0.01278507]\n",
      "Vector size: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize sentences\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=2\n",
    ")\n",
    "\n",
    "\n",
    "word = \"language\"\n",
    "print(f\"Embedding vector for '{word}':\")\n",
    "print(w2v_model.wv[word])\n",
    "print(\"Vector size:\", len(w2v_model.wv[word]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
